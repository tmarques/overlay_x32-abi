--- a/Zend/zend_operators.h	2016-06-22 02:50:40.000000000 +0100
+++ b/Zend/zend_operators.h	2016-07-12 17:02:18.000000000 +0100
@@ -510,26 +510,26 @@
 static zend_always_inline int fast_increment_function(zval *op1)
 {
 	if (EXPECTED(Z_TYPE_P(op1) == IS_LONG)) {
-#if defined(__GNUC__) && defined(__i386__)
-		__asm__(
+#if defined(__GNUC__) && ( defined(__i386__) || ( defined(__x86_64__) && defined(__ILP32__) ) ) /* i386 or un-optimized x32 */
+    __asm__(
 			"incl (%0)\n\t"
 			"jno  0f\n\t"
 			"movl $0x0, (%0)\n\t"
 			"movl $0x41e00000, 0x4(%0)\n\t"
-			"movb %1, %c2(%0)\n"
+			"movl %1, %c2(%0)\n"
 			"0:"
 			:
 			: "r"(&op1->value),
 			  "n"(IS_DOUBLE),
 			  "n"(ZVAL_OFFSETOF_TYPE)
 			: "cc");
-#elif defined(__GNUC__) && defined(__x86_64__)
+#elif defined(__GNUC__) && defined(__x86_64__) && defined(__LP64__) /* amd64 */
 		__asm__(
 			"incq (%0)\n\t"
 			"jno  0f\n\t"
 			"movl $0x0, (%0)\n\t"
 			"movl $0x43e00000, 0x4(%0)\n\t"
-			"movb %1, %c2(%0)\n"
+			"movl %1, %c2(%0)\n"
 			"0:"
 			:
 			: "r"(&op1->value),
@@ -553,26 +553,26 @@
 static zend_always_inline int fast_decrement_function(zval *op1)
 {
 	if (EXPECTED(Z_TYPE_P(op1) == IS_LONG)) {
-#if defined(__GNUC__) && defined(__i386__)
+#if defined(__GNUC__) && ( defined(__i386__) || ( defined(__x86_64__) && defined(__ILP32__) ) ) /* i386 or un-optimized x32 */
 		__asm__(
 			"decl (%0)\n\t"
 			"jno  0f\n\t"
 			"movl $0x00200000, (%0)\n\t"
 			"movl $0xc1e00000, 0x4(%0)\n\t"
-			"movb %1,%c2(%0)\n"
+			"movl %1,%c2(%0)\n"
 			"0:"
 			:
 			: "r"(&op1->value),
 			  "n"(IS_DOUBLE),
 			  "n"(ZVAL_OFFSETOF_TYPE)
 			: "cc");
-#elif defined(__GNUC__) && defined(__x86_64__)
+#elif defined(__GNUC__) && defined(__x86_64__) && defined(__LP64__) /* amd64 */
 		__asm__(
 			"decq (%0)\n\t"
 			"jno  0f\n\t"
 			"movl $0x00000000, (%0)\n\t"
 			"movl $0xc3e00000, 0x4(%0)\n\t"
-			"movb %1,%c2(%0)\n"
+			"movl %1,%c2(%0)\n"
 			"0:"
 			:
 			: "r"(&op1->value),
@@ -597,19 +597,19 @@
 {
 	if (EXPECTED(Z_TYPE_P(op1) == IS_LONG)) {
 		if (EXPECTED(Z_TYPE_P(op2) == IS_LONG)) {
-#if defined(__GNUC__) && defined(__i386__)
+#if defined(__GNUC__) && ( defined(__i386__) || ( defined(__x86_64__) && defined(__ILP32__) ) ) // i386 or un-optimized x32
 		__asm__(
 			"movl	(%1), %%eax\n\t"
 			"addl   (%2), %%eax\n\t"
 			"jo     0f\n\t"     
 			"movl   %%eax, (%0)\n\t"
-			"movb   %3, %c5(%0)\n\t"
+			"movl   %3, %c5(%0)\n\t"
 			"jmp    1f\n"
 			"0:\n\t"
 			"fildl	(%1)\n\t"
 			"fildl	(%2)\n\t"
 			"faddp	%%st, %%st(1)\n\t"
-			"movb   %4, %c5(%0)\n\t"
+			"movl   %4, %c5(%0)\n\t"
 			"fstpl	(%0)\n"
 			"1:"
 			: 
@@ -620,19 +620,19 @@
 			  "n"(IS_DOUBLE),
 			  "n"(ZVAL_OFFSETOF_TYPE)
 			: "eax","cc");
-#elif defined(__GNUC__) && defined(__x86_64__)
+#elif defined(__GNUC__) && defined(__x86_64__) && defined(__LP64__) /* amd64 */
 		__asm__(
 			"movq	(%1), %%rax\n\t"
 			"addq   (%2), %%rax\n\t"
 			"jo     0f\n\t"     
 			"movq   %%rax, (%0)\n\t"
-			"movb   %3, %c5(%0)\n\t"
+			"movl   %3, %c5(%0)\n\t"
 			"jmp    1f\n"
 			"0:\n\t"
 			"fildq	(%1)\n\t"
 			"fildq	(%2)\n\t"
 			"faddp	%%st, %%st(1)\n\t"
-			"movb   %4, %c5(%0)\n\t"
+			"movl   %4, %c5(%0)\n\t"
 			"fstpl	(%0)\n"
 			"1:"
 			: 
@@ -683,13 +683,13 @@
 {
 	if (EXPECTED(Z_TYPE_P(op1) == IS_LONG)) {
 		if (EXPECTED(Z_TYPE_P(op2) == IS_LONG)) {
-#if defined(__GNUC__) && defined(__i386__)
+#if defined(__GNUC__) && ( defined(__i386__) || ( defined(__x86_64__) && defined(__ILP32__) ) ) /* i386 or un-optimized x32 */
 		__asm__(
 			"movl	(%1), %%eax\n\t"
 			"subl   (%2), %%eax\n\t"
 			"jo     0f\n\t"     
 			"movl   %%eax, (%0)\n\t"
-			"movb   %3, %c5(%0)\n\t"
+			"movl   %3, %c5(%0)\n\t"
 			"jmp    1f\n"
 			"0:\n\t"
 			"fildl	(%2)\n\t"
@@ -699,7 +699,7 @@
 #else
 			"fsubp	%%st, %%st(1)\n\t"
 #endif
-			"movb   %4, %c5(%0)\n\t"
+			"movl   %4, %c5(%0)\n\t"
 			"fstpl	(%0)\n"
 			"1:"
 			: 
@@ -710,13 +710,13 @@
 			  "n"(IS_DOUBLE),
 			  "n"(ZVAL_OFFSETOF_TYPE)
 			: "eax","cc");
-#elif defined(__GNUC__) && defined(__x86_64__)
+#elif defined(__GNUC__) && defined(__x86_64__) && defined(__LP64__) /* amd64 */
 		__asm__(
 			"movq	(%1), %%rax\n\t"
 			"subq   (%2), %%rax\n\t"
 			"jo     0f\n\t"     
 			"movq   %%rax, (%0)\n\t"
-			"movb   %3, %c5(%0)\n\t"
+			"movl   %3, %c5(%0)\n\t"
 			"jmp    1f\n"
 			"0:\n\t"
 			"fildq	(%2)\n\t"
@@ -726,7 +726,7 @@
 #else
 			"fsubp	%%st, %%st(1)\n\t"
 #endif
-			"movb   %4, %c5(%0)\n\t"
+			"movl   %4, %c5(%0)\n\t"
 			"fstpl	(%0)\n"
 			"1:"
 			: 
